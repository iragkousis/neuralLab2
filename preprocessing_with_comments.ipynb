{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Εργαστηριακή Άσκηση 2. Μη επιβλεπόμενη μάθηση.\n",
        "#### Ομάδα 56\n",
        "#### Γεωργία Μανιφάβα Hi!\n",
        "#### Δημήτρης Βάσιος\n",
        "#### Ηλίας Ραγκούσης\n",
        "## Σύστημα συστάσεων βασισμένο στο περιεχόμενο\n",
        "## Σημασιολογική απεικόνιση δεδομένων με χρήση SOM \n",
        "Ημερομηνία εκφώνησης της άσκησης: 29 Νοεμβρίου 2022\n",
        "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n",
        "dddd\n",
        "αααααααααααααααα\n",
        "**Θα βρείτε το παρόν σε μορφή jupyter notebook ως συνημμένο στο τέλος της εκφώνησης.**\n"
      ],
      "metadata": {
        "id": "hLFXD3IePSyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install h5py\n",
        "!pip install typing-extensions\n",
        "!pip install wheel\n",
        "!pip install --upgrade numpy\n",
        "!pip install --upgrade pandas\n",
        "!pip install --upgrade nltk\n",
        "!pip install --upgrade scikit-learn\n",
        "!pip install --upgrade joblib\n",
        "!pip install --upgrade contractions\n",
        "!pip install unidecode"
      ],
      "metadata": {
        "id": "S5wbBzIYnird",
        "execution": {
          "iopub.status.busy": "2022-12-30T16:25:59.136733Z",
          "iopub.execute_input": "2022-12-30T16:25:59.137338Z",
          "iopub.status.idle": "2022-12-30T16:27:32.802477Z",
          "shell.execute_reply.started": "2022-12-30T16:25:59.137304Z",
          "shell.execute_reply": "2022-12-30T16:27:32.801054Z"
        },
        "trusted": true,
        "outputId": "0556e5fd-2d64-42b1-a9e5-9fcac94604b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (22.3.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (3.7.0)\nRequirement already satisfied: numpy>=1.14.5 in /opt/conda/lib/python3.7/site-packages (from h5py) (1.21.6)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (4.4.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (0.37.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (1.21.6)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.3.5)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2022.1)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (1.21.6)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (3.8)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk) (8.0.4)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk) (1.2.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk) (4.64.0)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.7/site-packages (from nltk) (2021.11.10)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->nltk) (4.13.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.8.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (4.4.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (1.0.2)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.2.0)\nRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.7.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (3.1.0)\nRequirement already satisfied: numpy>=1.14.6 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.21.6)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (1.2.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: contractions in /opt/conda/lib/python3.7/site-packages (0.1.73)\nRequirement already satisfied: textsearch>=0.0.21 in /opt/conda/lib/python3.7/site-packages (from contractions) (0.0.24)\nRequirement already satisfied: anyascii in /opt/conda/lib/python3.7/site-packages (from textsearch>=0.0.21->contractions) (0.3.1)\nRequirement already satisfied: pyahocorasick in /opt/conda/lib/python3.7/site-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: unidecode in /opt/conda/lib/python3.7/site-packages (1.3.4)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Εισαγωγή του Dataset"
      ],
      "metadata": {
        "id": "aViHqlQcPSyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Το σύνολο δεδομένων με το οποίο θα δουλέψουμε είναι βασισμένο στο [Carnegie Mellon Movie Summary Corpus](http://www.cs.cmu.edu/~ark/personas/). Πρόκειται για ένα dataset με 22.301 περιγραφές ταινιών. Η περιγραφή κάθε ταινίας αποτελείται από τον τίτλο της, μια ή περισσότερες ετικέτες που χαρακτηρίζουν το είδος της ταινίας και τέλος τη σύνοψη της υπόθεσής της. Αρχικά εισάγουμε το dataset (χρησιμοποιήστε αυτούσιο τον κώδικα, δεν χρειάζεστε το αρχείο csv) στο dataframe `df_data_1`: "
      ],
      "metadata": {
        "id": "2ZVmdDExPSyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset_url = \"https://drive.google.com/uc?export=download&id=1zo13kUAf-MDMPZmBDxq1FxWtZY01lsxD\"\n",
        "df_data_1 = pd.read_csv(dataset_url, sep='\\t',  header=None, quoting=3)"
      ],
      "metadata": {
        "id": "62SOj46gPSyS",
        "execution": {
          "iopub.status.busy": "2022-12-30T16:27:32.806803Z",
          "iopub.execute_input": "2022-12-30T16:27:32.807259Z",
          "iopub.status.idle": "2022-12-30T16:27:35.313920Z",
          "shell.execute_reply.started": "2022-12-30T16:27:32.807177Z",
          "shell.execute_reply": "2022-12-30T16:27:35.312763Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Κάθε ομάδα θα δουλέψει σε **ένα μοναδικό υποσύνολο 5.000 ταινιών** (διαφορετικό dataset για κάθε ομάδα) ως εξής:\n",
        "\n",
        "1. Κάθε ομάδα του εργαστηρίου νευρωνικών έχει έναν αριθμό στο helios. Θα βάλετε τον αριθμό αυτό στη μεταβλητή team_seed_number στο επόμενο κελί κώδικα.\n",
        "\n",
        "2. Το data frame `df_data_2` έχει γραμμές όσες και οι ομάδες και 5.000 στήλες. Σε κάθε ομάδα αντιστοιχεί η γραμμή του πίνακα με το `team_seed_number` της. Η γραμμή αυτή θα περιλαμβάνει 5.000 διαφορετικούς αριθμούς που αντιστοιχούν σε ταινίες του αρχικού dataset. \n",
        "\n",
        "3. Τρέξτε τον κώδικα. Θα προκύψουν τα μοναδικά για κάθε ομάδα  titles, categories, catbins, summaries και corpus με τα οποία θα δουλέψετε."
      ],
      "metadata": {
        "id": "7TAEZGdIPSyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Στο επόμενη γραμή βάλτε τον αριθμό της ομάδας στο εργαστήριο των νευρωνικών\n",
        "team_seed_number = 56\n",
        "\n",
        "movie_seeds_url = \"https://drive.google.com/uc?export=download&id=1g6F4TCHrs2wgtdOk7D3gtONaeirNt_Vo\"\n",
        "df_data_2 = pd.read_csv(movie_seeds_url, header=None)\n",
        "\n",
        "# επιλέγεται \n",
        "my_index = df_data_2.iloc[team_seed_number,:].values\n",
        "\n",
        "titles = df_data_1.iloc[:, [2]].values[my_index] # movie titles (string)\n",
        "categories = df_data_1.iloc[:, [3]].values[my_index] # movie categories (string)\n",
        "bins = df_data_1.iloc[:, [4]]\n",
        "catbins = bins[4].str.split(',', expand=True).values.astype(float)[my_index] # movie categories in binary form (1 feature per category)\n",
        "summaries =  df_data_1.iloc[:, [5]].values[my_index] # movie summaries (string)\n",
        "corpus = summaries[:,0].tolist() # list form of summaries\n",
        "corpus_df = pd.DataFrame(corpus) # dataframe version of corpus"
      ],
      "metadata": {
        "id": "2POlqDjkPSyY",
        "execution": {
          "iopub.status.busy": "2022-12-30T16:27:35.315733Z",
          "iopub.execute_input": "2022-12-30T16:27:35.316010Z",
          "iopub.status.idle": "2022-12-30T16:27:38.090620Z",
          "shell.execute_reply.started": "2022-12-30T16:27:35.315986Z",
          "shell.execute_reply": "2022-12-30T16:27:38.089661Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Ο πίνακας **titles** περιέχει τους τίτλους των ταινιών. Παράδειγμα: 'Sid and Nancy'.\n",
        "- O πίνακας **categories** περιέχει τις κατηγορίες (είδη) της ταινίας υπό τη μορφή string. Παράδειγμα: '\"Tragedy\",  \"Indie\",  \"Punk rock\",  \"Addiction Drama\",  \"Cult\",  \"Musical\",  \"Drama\",  \"Biopic \\[feature\\]\",  \"Romantic drama\",  \"Romance Film\",  \"Biographical film\"'. Παρατηρούμε ότι είναι μια comma separated λίστα strings, με κάθε string να είναι μια κατηγορία.\n",
        "- Ο πίνακας **catbins** περιλαμβάνει πάλι τις κατηγορίες των ταινιών αλλά σε δυαδική μορφή ([one hot encoding](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f)). Έχει διαστάσεις 5.000 x 322 (όσες οι διαφορετικές κατηγορίες). Αν η ταινία ανήκει στο συγκεκριμένο είδος η αντίστοιχη στήλη παίρνει την τιμή 1, αλλιώς παίρνει την τιμή 0.\n",
        "- Ο πίνακας **summaries** και η λίστα **corpus** περιλαμβάνουν τις συνόψεις των ταινιών (η corpus είναι απλά ο summaries σε μορφή λίστας). Κάθε σύνοψη είναι ένα (συνήθως μεγάλο) string. Παράδειγμα: *'The film is based on the real story of a Soviet Internal Troops soldier who killed his entire unit  as a result of Dedovschina. The plot unfolds mostly on board of the prisoner transport rail car guarded by a unit of paramilitary conscripts.'*\n",
        "- το dataframe **corpus_df** που είναι απλά το corpus σε μορφή dataframe. Τα summaries βρίσκονται στην κολόνα 0. Πιθανώς να σας βολεύει να κάνετε κάποιες προεπεξεργασίες με dataframes.\n",
        "\n",
        "\n",
        "Θεωρούμε ως **ID** της κάθε ταινίας τον αριθμό γραμμής της ή το αντίστοιχο στοιχείο της λίστας. Παράδειγμα: για να τυπώσουμε τη σύνοψη της ταινίας με `ID=999` (την χιλιοστή) θα γράψουμε `print(corpus[999])`."
      ],
      "metadata": {
        "id": "If66lkwxPSyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ID = 999\n",
        "print(titles[ID])\n",
        "print(categories[ID])\n",
        "print(catbins[ID])\n",
        "print(corpus[ID])\n",
        "print(corpus_df.loc[ID,:])\n"
      ],
      "metadata": {
        "id": "k_7A3KXLp0qS",
        "execution": {
          "iopub.status.busy": "2022-12-30T16:27:38.093548Z",
          "iopub.execute_input": "2022-12-30T16:27:38.094000Z",
          "iopub.status.idle": "2022-12-30T16:27:38.105170Z",
          "shell.execute_reply.started": "2022-12-30T16:27:38.093964Z",
          "shell.execute_reply": "2022-12-30T16:27:38.104198Z"
        },
        "trusted": true,
        "outputId": "559f9ecc-d265-47b6-faa1-e15cc482ebd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "['Aina']\n['\"Musical\",  \"Drama\",  \"Romance Film\"']\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\nAina is a love story of two hearts and two souls but from two different social classes, one being a daughter of a business tycoon i.e. Rita  and one being a realistic, self-confident and a little bit arrogant poor young man . Rita is a leisure girl whereas Nadeem works as a hotel receptionist and they both fall in love. The differences between Nadeem and Rita's father Seth  arises right from the beginning, esp. when he  criticised Nadeem for his social status and earning capacity, saying his daughter's sari costs 7,500 rupees as compare to his  monthly salary of 750 rupees. But after a struggle by Rita and a threat to her parents that she would suicide if she would not be allowed to marry Nadeem, her father agreed. So both married but Nadeem knew that Seth was not really happy with this marriage. One day Rita's mother  came to her daughter's house, located in a middle class, and offered Nadeem a job in Seth's friend's business firm, which he denied. Not only this, he also angried on Rita about the telephone  installed in his house by her. A few days later, Rita's mother purchased furniture for Nadeem's house  and invited her guests to a dance party in Nadeem's house. When Nadeem came back home, he became very angry and he asked Rita to leave the house and stay in her father's house. Rita left, with tears, Nadeem's house. Next day, Nadeem got an appointment from a bigger company perhaps from a hotel in Murree, so he has to leave the city immediately. But before leaving the city he tried to meet Rita, but, at the entrance of her house, he met Seth. Seth misinform him that Rita does not want to see his face again and decided to get divorce from him, as she has realised that she has made such a big mistake in marrying a poor man like Nadeem. Shocked and dishearted Nadeem left the city. On the other side, Rita was waiting for him in the hope that he would come back and take her back to his home. But her father deceived her too that he had visited Nadeem but he insulted him in front of his friends and said that he is going to divorce her. This shocked Rita to such an extent that she could not bear the pain of pregnancy. So they moved to a hospital in the same city where the Nadeem was gone for his new job. During his journey to Murree, a car driver incidentally injured Nadeem. Nadeem moved to the same hospital where Rita was admitted for child delivery. In the hospital's lobby, he found his wife unconscious on the hospital bed, during this Seth interrupted and said that he is going to give away the child to an orphanage, but Nadeem refused to do that and said that he will take care of child himself. The film has taken a very sad turn at that point esp. when Nadeem carrying the baby to his house and singing the sad version of the song Mujhe dil se na bhulana... and remembering those happy days with Rita. On the other side, Seth lied not only to Rita but to his wife  also that the baby has died. This was a tremendeous shock for Rita and she somewhat lost her mental balance. After few years the baby is grown up to a teenage boy , a very sensitive and very anxious about his mother. Whereas, Rita's mental instability grew day by day, so her mother asked Rita's father to let Rita visit her baby's grave which would perhaps help her. Next day all visited  and searched the graveyard, where they come across the real baby , who was picking flowers there. As there was nothing to found there, they started leaving the place. During this Shahzeb immediately reached home and asked his father  about the features of his mother, Nadeem hand him over her photo. Shahzeb recognised his mother and ran towards the road where Rita and her parents come from and started singing the same song which he used to listen from his father Mujhe dil se na bhulana... Rita stopped the car and ran towards the place where the voice was coming from and finally she reached Nadeem's house. Amazed by the situation both asked about the divorce, as both were in misunderstanding, the truth revealed that it was Rita's father who had actually planned their separation  and who had lied to her that her child is dead. Rita turned angriely and slapped her embarrassed father, who was listening this. Her mother said at this moment: 'You have not slapped your father, but you have actually slapped that mentality that believes in differences between poor and rich'. And then they lived happily ever after.\n0    Aina is a love story of two hearts and two sou...\nName: 999, dtype: object\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Εφαρμογή 1. Υλοποίηση συστήματος συστάσεων ταινιών βασισμένο στο περιεχόμενο\n",
        "<img src=\"http://clture.org/wp-content/uploads/2015/12/Netflix-Streaming-End-of-Year-Posts.jpg\" width=\"70%\">"
      ],
      "metadata": {
        "id": "UTNgwBfjPSyc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Η πρώτη εφαρμογή που θα αναπτύξετε θα είναι ένα [σύστημα συστάσεων](https://en.wikipedia.org/wiki/Recommender_system) ταινιών βασισμένο στο περιεχόμενο (content based recommender system). Τα συστήματα συστάσεων στοχεύουν στο να προτείνουν αυτόματα στο χρήστη αντικείμενα από μια συλλογή τα οποία ιδανικά θέλουμε να βρει ενδιαφέροντα ο χρήστης. Η κατηγοριοποίηση των συστημάτων συστάσεων βασίζεται στο πώς γίνεται η επιλογή (filtering) των συστηνόμενων αντικειμένων. Οι δύο κύριες κατηγορίες είναι η συνεργατική διήθηση (collaborative filtering) όπου το σύστημα προτείνει στο χρήστη αντικείμενα που έχουν αξιολογηθεί θετικά από χρήστες που έχουν παρόμοιο με αυτόν ιστορικό αξιολογήσεων και η διήθηση με βάση το περιεχόμενο (content based filtering), όπου προτείνονται στο χρήστη αντικείμενα με παρόμοιο περιεχόμενο (με βάση κάποια χαρακτηριστικά) με αυτά που έχει προηγουμένως αξιολογήσει θετικά.\n",
        "\n",
        "Το σύστημα συστάσεων που θα αναπτύξετε θα βασίζεται στο **περιεχόμενο** και συγκεκριμένα στις συνόψεις των ταινιών (corpus). \n"
      ],
      "metadata": {
        "id": "rnA2RP8GPSyf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Προεπεξεργασία\n",
        "\n",
        "Το πρώτο βήμα στην επεξεργασία μας είναι ο καθαρισμός των περιγραφών των ταινιών. \n",
        "\n",
        "Εκτυπώστε (αρκετές) διαφορετικές περιγραφές ταινιών για να δείτε πιθανά προβλήματα που θα πρέπει να αντιμετωπιστούν.\n",
        "\n",
        "Τα (ελάχιστα) βήματα καθαρισμού που προτείνουμε είναι:\n",
        "- μετατροπή όλων των χαρακτήρων σε πεζά,\n",
        "- αφαίρεση των stopwords. Εδώ σημειώστε ότι για το δεδομένο task του συστήματος συστάσεων που είναι η πρόταση ταινιών ίσως θα είχαν ενδιαφέρον και λίστες stopwords πέραν αυτών της κοινής γλώσσας.\n",
        "- αφαίρεση σημείων στίξης και ειδικών χαρακτρήρων (special characters). Αυτό δεν γίνεται μόνο με την punkt του NLTK. Θα μπορούσατε να βασιστείτε σε κανονικές εκφράσεις (regular expressions), και\n",
        "- αφαίρεση πολυ σύντομων συμβολοσειρών.\n",
        "\n",
        "Προσοχή: το corpus και τα τελικά tokens που θα το αποτελούν θα χρησιμοποιηθούν στη συνέχεια ως κλειδιά για να βρούμε εμφυτεύματα. Για το λόγο αυτό, πρέπει να είστε προσεκτικοί ως προς την εφαρμογή μεθόδων κανονικοποίησης (text normalization) όπως το stemming και το lemmatization."
      ],
      "metadata": {
        "id": "l3nZv-xueEtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#necessary imports and downloads\n",
        "import numpy as np\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from unidecode import unidecode\n",
        "import collections\n",
        "import contractions\n",
        "import re\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords') \n",
        "nltk.download('wordnet') \n",
        "nltk.download('rslp')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-30T16:27:38.106415Z",
          "iopub.execute_input": "2022-12-30T16:27:38.106745Z",
          "iopub.status.idle": "2022-12-30T16:27:38.119726Z",
          "shell.execute_reply.started": "2022-12-30T16:27:38.106716Z",
          "shell.execute_reply": "2022-12-30T16:27:38.118345Z"
        },
        "trusted": true,
        "id": "53Qmlzo7rvxW",
        "outputId": "40e5b603-2f6c-4ea4-bb06-79d8f71a00b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package rslp to /usr/share/nltk_data...\n[nltk_data]   Package rslp is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n",
          "output_type": "stream"
        },
        {
          "execution_count": 34,
          "output_type": "execute_result",
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Oρισμός της thorough_filter με όρισμα τις λέξεις ενός corpus. Την χρησιμοποιούμε παρακάτω στην preprocessing για να αφαιρέσουμε τα tokens που περιέχουν περισσότερα από ένα σημεία στίξης.*"
      ],
      "metadata": {
        "id": "b3s3vzdjrvxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def thorough_filter(words):\n",
        "    filtered_words = []\n",
        "    for word in words:\n",
        "        pun = []\n",
        "        for letter in word:\n",
        "            pun.append(letter in string.punctuation)\n",
        "        if not all(pun):\n",
        "            filtered_words.append(word)\n",
        "    return filtered_words\n",
        "\n",
        "print(string.punctuation)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-30T16:27:38.121202Z",
          "iopub.execute_input": "2022-12-30T16:27:38.121501Z",
          "iopub.status.idle": "2022-12-30T16:27:38.129372Z",
          "shell.execute_reply.started": "2022-12-30T16:27:38.121473Z",
          "shell.execute_reply": "2022-12-30T16:27:38.128082Z"
        },
        "trusted": true,
        "id": "cQGQKnDjrvxY",
        "outputId": "32c5d09e-958a-49da-9849-7eb48ee9af04"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Erase names\n",
        "*Θα θεωρήσουμε ως επιπλέον stopwords, τα συνηθισμένα αγγλικά αντρικά και γυναικεία ονόματα. Οι ταινίες συνήθως περιέχουν τα ονόματα πρωταγωνιστών. Κάνουμε αυτή την επιλογή ώστε το σύστημα συστάσεων να μην προτείνει ταινίες με άσχετο περιεχόμενο επειδή απλά τυγχάνει οι πρωταγωνιστές να έχουν το ίδιο όνομα σε δύο διαφορετικές ταινίες. Χρησιμοποιούμε το αρχείο names.txt που έχει αντληθεί από το United States Naval Academy και περιέχει αγγλικά ονόματα. Πριν το χρησιμοποιήσουμε για την προεπεξεργασία, μετατρέπουμε το πρώτο γράμμα των ονομάτων σε πεζό. Η διαδικασία φαίνεται στο παρακάτω κελί.*"
      ],
      "metadata": {
        "id": "8ZeSfI92rvxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"/kaggle/input/namestxt/names.txt\", \"r\")\n",
        "names = list(f.read().splitlines())\n",
        "\n",
        "for i in range (len(names)):\n",
        "    names[i] = names[i].lower()   "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-30T16:27:38.131449Z",
          "iopub.execute_input": "2022-12-30T16:27:38.131932Z",
          "iopub.status.idle": "2022-12-30T16:27:38.151127Z",
          "shell.execute_reply.started": "2022-12-30T16:27:38.131871Z",
          "shell.execute_reply": "2022-12-30T16:27:38.149967Z"
        },
        "trusted": true,
        "id": "dnedeGXhrvxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Παρακάτω ορίζουμε την συνάρτηση προεπεξεργασίας που διατρέχει ολόκληρο το dataset, για να εκτελεστούν παντού τα βήματα καθαρισμού. Παρακάτω παραθέτουμε με σειρά εκτέλεσης τα βήματα προεπεξεργασίας:*\n",
        "* Πριν εξάγουμε τα tokens, κάνουμε μια ελάχιστη προεπεξεργασία, αφαιρώντας τα πιθανά κενά στην αρχή και στο τέλος των λέξεων, μετατρέπουμε όλες τις λέξεις σε πεζά και διορθώνουμε τις συντομεύσεις φράσεων(don't-->do not)\n",
        "* Eξάγουμε τα tokens\n",
        "* Επίσης, αφαιρούμε τα stopwords που δεν προσφέρουν χρήσιμη πληροφορία για τη διάκριση περιεχομένου. Κρίναμε σημαντική και την αφαίρεση ειδικών stopwords που χρησιμοποιούνται αρκετά συχνά στο συγκεκριμένο dataset αλλά και γενικά στις περιγραφές ταινιών και έτσι ορίσαμε την λίστα special_stopwords. Eνοποιούμε τις λίστες common_stopwords και special_stopwords και γεμίζουμε την filtered_words με τις λέξεις που δεν είναι ούτε stopwords, oύτε αγγλικά ονόματα αλλά ούτε και σημεία στίξης.\n",
        "* Χρησιμοποιούμε τη thorough_filter για περεταίρω φιλτράρισμα στα σημεία στίξης. Αντικαθιστούμε τους special non-ascii characters, που συνήθως είναι τονισμοί ξένων γλωσσών (unidecode). Ακόμα, αντιμετωπίζουμε οποιονδήποτε άλλον ειδικό χαρακτήρα με regular expression(πχ \\n - Newline και \\t- Horizontal tab)\n",
        "* Αφαιρούμε τις πολύ σύντομες λέξεις(μήκους μικρότερου του 3)\n",
        "* Eφαρμόζουμε τον γλωσσικό μεταχηματισμό του Lemmatization, ο οποίος αν και πιο αργός από το stemming, προσφέρει πιο ακριβή αποτελέσματα.\n"
      ],
      "metadata": {
        "id": "Nq0oaQPHrvxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(corpus):\n",
        "    #strip spaces, convert letters to lower, fix contractions:you're-->you are\n",
        "    for i in range (len(corpus)):\n",
        "        corpus[i] = corpus[i].strip()  # strip leading / trailing spaces\n",
        "        corpus[i] = corpus[i].lower()\n",
        "        corpus[i] = contractions.fix(corpus[i])\n",
        "    \n",
        "    #tokenization, To tokens[i] είναι τα tokens του corpus[i]\n",
        "    tokens = []\n",
        "    for i in range (len(corpus)):\n",
        "        words = nltk.word_tokenize(corpus[i])\n",
        "        tokens.append(words)\n",
        "    \n",
        "    #Remove stopwords(general english ones and some special ones), the punctuation symbols and the names \n",
        "    common_stopwords = set(stopwords.words('english'))\n",
        "    special_stopwords=[\"film\",\"movie\",\"plot\",\"performance\",\"episode\",\"story\",\"scene\",\"actor\",\"summary\",\"hero\",\"protagonist\",\"finale\",\"prologue\", \"director\",\"character\",\"cinema\",\n",
        "                      \"one\",\"two\",\"love\",\"man\",\"father\",\"new\",\"young\",\"life\",\"time\",\"family\",\"first\",\"police\",\"home\",\"mother\",\"house\",\"wife\",\"three\",\"money\",\"car\",\n",
        "                      \"son\",\"friend\",\"man\",\"woman\",\"girl\",\"friend\",\"death\",\"Dr.\",\"Mr.\",\"Mrs\",\"people\",\"town\",\"fight\",\"brother\",\"place\",\"parents\"]      \n",
        "    \n",
        "    StopWords=set.union(common_stopwords, special_stopwords)\n",
        "    #print(StopWords)\n",
        "    \n",
        "    #Στο filtered_words[i] γίνονται append οι λέξεις του corpus[i] που έχουν περάσει το filtering, που άρα δεν είναι σημεία στίξης\n",
        "    #και δεν ανήκουν ούτε στα stopwords ούτε στα αγγλικά ονόματα.\n",
        "    filtered_words=[]\n",
        "    for i in range (len(corpus)): \n",
        "        word=[w for w in tokens[i] if (w not in StopWords and w not in list(string.punctuation) and w not in names)]\n",
        "        filtered_words.append(word)\n",
        "    \n",
        "    #Extra hanling of punctuation with thorough_filter, remove non ascii characters and special characters with regular expression\n",
        "    for i in range (len(corpus)):\n",
        "        filtered_words[i] = thorough_filter(filtered_words[i])\n",
        "        filtered_words[i] = [unidecode(w) for w in filtered_words[i]]\n",
        "        filtered_words[i] = [re.sub('[^a-zA-Z0-9-_]', '', w) for w in filtered_words[i]] # keep only lowercase letters,numbers and underscores\n",
        "    \n",
        "    #Remove short words\n",
        "    filtered_words_new = []\n",
        "    for i in range (len(corpus)):\n",
        "        word=[w for w in filtered_words[i] if (len(w)>=3)]\n",
        "        filtered_words_new.append(word)\n",
        "        \n",
        "    #lemmatization\n",
        "    lem_words=[]\n",
        "    for i in range(len(corpus)):\n",
        "        lem_words.append([WordNetLemmatizer().lemmatize(w) for w in filtered_words_new[i]])\n",
        "        \n",
        "    # concatenate tokens back to a sentence\n",
        "    pr_corpus = []\n",
        "    for i in range (len(corpus)):\n",
        "        pr_corpus.append(\" \".join(lem_words[i]))\n",
        "\n",
        "    \n",
        "    return corpus,tokens,lem_words,pr_corpus"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-30T16:27:38.152975Z",
          "iopub.execute_input": "2022-12-30T16:27:38.153370Z",
          "iopub.status.idle": "2022-12-30T16:27:38.173493Z",
          "shell.execute_reply.started": "2022-12-30T16:27:38.153333Z",
          "shell.execute_reply": "2022-12-30T16:27:38.171874Z"
        },
        "trusted": true,
        "id": "pORfAq97rvxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Eφαρμόζουμε τη συνάρτηση προεπεξεργασίας στο corpus, που είναι η λίστα με όλες τις περιγραφές των ταινιών του dataset σε μορφή λίστας που περιέχει strings. Τυπώνουμε για έλεγχο ένα παράδειγμα με τα tokens στην αρχή της προεπεξεργασίας, τα tokens στο τέλος της προεπεξεργασίας και το προεπεξεργασμένο corpus με συνένωση των tokens.*"
      ],
      "metadata": {
        "id": "5PODrZUgrvxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pr_corpus=preprocessing(corpus)\n",
        "print(pr_corpus[1][ID])\n",
        "print(pr_corpus[2][ID])\n",
        "print(pr_corpus[3][ID])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-12-30T16:27:38.175259Z",
          "iopub.execute_input": "2022-12-30T16:27:38.175670Z",
          "iopub.status.idle": "2022-12-30T16:31:40.444458Z",
          "shell.execute_reply.started": "2022-12-30T16:27:38.175640Z",
          "shell.execute_reply": "2022-12-30T16:31:40.443752Z"
        },
        "trusted": true,
        "id": "YCRVZuworvxc",
        "outputId": "99b90359-9a86-4921-b519-4382cd75f930"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "['aina', 'is', 'a', 'love', 'story', 'of', 'two', 'hearts', 'and', 'two', 'souls', 'but', 'from', 'two', 'different', 'social', 'classes', ',', 'one', 'being', 'a', 'daughter', 'of', 'a', 'business', 'tycoon', 'i.e', '.', 'rita', 'and', 'one', 'being', 'a', 'realistic', ',', 'self-confident', 'and', 'a', 'little', 'bit', 'arrogant', 'poor', 'young', 'man', '.', 'rita', 'is', 'a', 'leisure', 'girl', 'whereas', 'nadeem', 'works', 'as', 'a', 'hotel', 'receptionist', 'and', 'they', 'both', 'fall', 'in', 'love', '.', 'the', 'differences', 'between', 'nadeem', 'and', 'rita', \"'s\", 'father', 'seth', 'arises', 'right', 'from', 'the', 'beginning', ',', 'esp', '.', 'when', 'he', 'criticised', 'nadeem', 'for', 'his', 'social', 'status', 'and', 'earning', 'capacity', ',', 'saying', 'his', 'daughter', \"'s\", 'sari', 'costs', '7,500', 'rupees', 'as', 'compare', 'to', 'his', 'monthly', 'salary', 'of', '750', 'rupees', '.', 'but', 'after', 'a', 'struggle', 'by', 'rita', 'and', 'a', 'threat', 'to', 'her', 'parents', 'that', 'she', 'would', 'suicide', 'if', 'she', 'would', 'not', 'be', 'allowed', 'to', 'marry', 'nadeem', ',', 'her', 'father', 'agreed', '.', 'so', 'both', 'married', 'but', 'nadeem', 'knew', 'that', 'seth', 'was', 'not', 'really', 'happy', 'with', 'this', 'marriage', '.', 'one', 'day', 'rita', \"'s\", 'mother', 'came', 'to', 'her', 'daughter', \"'s\", 'house', ',', 'located', 'in', 'a', 'middle', 'class', ',', 'and', 'offered', 'nadeem', 'a', 'job', 'in', 'seth', \"'s\", 'friend', \"'s\", 'business', 'firm', ',', 'which', 'he', 'denied', '.', 'not', 'only', 'this', ',', 'he', 'also', 'angried', 'on', 'rita', 'about', 'the', 'telephone', 'installed', 'in', 'his', 'house', 'by', 'her', '.', 'a', 'few', 'days', 'later', ',', 'rita', \"'s\", 'mother', 'purchased', 'furniture', 'for', 'nadeem', \"'s\", 'house', 'and', 'invited', 'her', 'guests', 'to', 'a', 'dance', 'party', 'in', 'nadeem', \"'s\", 'house', '.', 'when', 'nadeem', 'came', 'back', 'home', ',', 'he', 'became', 'very', 'angry', 'and', 'he', 'asked', 'rita', 'to', 'leave', 'the', 'house', 'and', 'stay', 'in', 'her', 'father', \"'s\", 'house', '.', 'rita', 'left', ',', 'with', 'tears', ',', 'nadeem', \"'s\", 'house', '.', 'next', 'day', ',', 'nadeem', 'got', 'an', 'appointment', 'from', 'a', 'bigger', 'company', 'perhaps', 'from', 'a', 'hotel', 'in', 'murree', ',', 'so', 'he', 'has', 'to', 'leave', 'the', 'city', 'immediately', '.', 'but', 'before', 'leaving', 'the', 'city', 'he', 'tried', 'to', 'meet', 'rita', ',', 'but', ',', 'at', 'the', 'entrance', 'of', 'her', 'house', ',', 'he', 'met', 'seth', '.', 'seth', 'misinform', 'him', 'that', 'rita', 'does', 'not', 'want', 'to', 'see', 'his', 'face', 'again', 'and', 'decided', 'to', 'get', 'divorce', 'from', 'him', ',', 'as', 'she', 'has', 'realised', 'that', 'she', 'has', 'made', 'such', 'a', 'big', 'mistake', 'in', 'marrying', 'a', 'poor', 'man', 'like', 'nadeem', '.', 'shocked', 'and', 'dishearted', 'nadeem', 'left', 'the', 'city', '.', 'on', 'the', 'other', 'side', ',', 'rita', 'was', 'waiting', 'for', 'him', 'in', 'the', 'hope', 'that', 'he', 'would', 'come', 'back', 'and', 'take', 'her', 'back', 'to', 'his', 'home', '.', 'but', 'her', 'father', 'deceived', 'her', 'too', 'that', 'he', 'had', 'visited', 'nadeem', 'but', 'he', 'insulted', 'him', 'in', 'front', 'of', 'his', 'friends', 'and', 'said', 'that', 'he', 'is', 'going', 'to', 'divorce', 'her', '.', 'this', 'shocked', 'rita', 'to', 'such', 'an', 'extent', 'that', 'she', 'could', 'not', 'bear', 'the', 'pain', 'of', 'pregnancy', '.', 'so', 'they', 'moved', 'to', 'a', 'hospital', 'in', 'the', 'same', 'city', 'where', 'the', 'nadeem', 'was', 'gone', 'for', 'his', 'new', 'job', '.', 'during', 'his', 'journey', 'to', 'murree', ',', 'a', 'car', 'driver', 'incidentally', 'injured', 'nadeem', '.', 'nadeem', 'moved', 'to', 'the', 'same', 'hospital', 'where', 'rita', 'was', 'admitted', 'for', 'child', 'delivery', '.', 'in', 'the', 'hospital', \"'s\", 'lobby', ',', 'he', 'found', 'his', 'wife', 'unconscious', 'on', 'the', 'hospital', 'bed', ',', 'during', 'this', 'seth', 'interrupted', 'and', 'said', 'that', 'he', 'is', 'going', 'to', 'give', 'away', 'the', 'child', 'to', 'an', 'orphanage', ',', 'but', 'nadeem', 'refused', 'to', 'do', 'that', 'and', 'said', 'that', 'he', 'will', 'take', 'care', 'of', 'child', 'himself', '.', 'the', 'film', 'has', 'taken', 'a', 'very', 'sad', 'turn', 'at', 'that', 'point', 'esp', '.', 'when', 'nadeem', 'carrying', 'the', 'baby', 'to', 'his', 'house', 'and', 'singing', 'the', 'sad', 'version', 'of', 'the', 'song', 'mujhe', 'dil', 'se', 'na', 'bhulana', '...', 'and', 'remembering', 'those', 'happy', 'days', 'with', 'rita', '.', 'on', 'the', 'other', 'side', ',', 'seth', 'lied', 'not', 'only', 'to', 'rita', 'but', 'to', 'his', 'wife', 'also', 'that', 'the', 'baby', 'has', 'died', '.', 'this', 'was', 'a', 'tremendeous', 'shock', 'for', 'rita', 'and', 'she', 'somewhat', 'lost', 'her', 'mental', 'balance', '.', 'after', 'few', 'years', 'the', 'baby', 'is', 'grown', 'up', 'to', 'a', 'teenage', 'boy', ',', 'a', 'very', 'sensitive', 'and', 'very', 'anxious', 'about', 'his', 'mother', '.', 'whereas', ',', 'rita', \"'s\", 'mental', 'instability', 'grew', 'day', 'by', 'day', ',', 'so', 'her', 'mother', 'asked', 'rita', \"'s\", 'father', 'to', 'let', 'rita', 'visit', 'her', 'baby', \"'s\", 'grave', 'which', 'would', 'perhaps', 'help', 'her', '.', 'next', 'day', 'all', 'visited', 'and', 'searched', 'the', 'graveyard', ',', 'where', 'they', 'come', 'across', 'the', 'real', 'baby', ',', 'who', 'was', 'picking', 'flowers', 'there', '.', 'as', 'there', 'was', 'nothing', 'to', 'found', 'there', ',', 'they', 'started', 'leaving', 'the', 'place', '.', 'during', 'this', 'shahzeb', 'immediately', 'reached', 'home', 'and', 'asked', 'his', 'father', 'about', 'the', 'features', 'of', 'his', 'mother', ',', 'nadeem', 'hand', 'him', 'over', 'her', 'photo', '.', 'shahzeb', 'recognised', 'his', 'mother', 'and', 'ran', 'towards', 'the', 'road', 'where', 'rita', 'and', 'her', 'parents', 'come', 'from', 'and', 'started', 'singing', 'the', 'same', 'song', 'which', 'he', 'used', 'to', 'listen', 'from', 'his', 'father', 'mujhe', 'dil', 'se', 'na', 'bhulana', '...', 'rita', 'stopped', 'the', 'car', 'and', 'ran', 'towards', 'the', 'place', 'where', 'the', 'voice', 'was', 'coming', 'from', 'and', 'finally', 'she', 'reached', 'nadeem', \"'s\", 'house', '.', 'amazed', 'by', 'the', 'situation', 'both', 'asked', 'about', 'the', 'divorce', ',', 'as', 'both', 'were', 'in', 'misunderstanding', ',', 'the', 'truth', 'revealed', 'that', 'it', 'was', 'rita', \"'s\", 'father', 'who', 'had', 'actually', 'planned', 'their', 'separation', 'and', 'who', 'had', 'lied', 'to', 'her', 'that', 'her', 'child', 'is', 'dead', '.', 'rita', 'turned', 'angriely', 'and', 'slapped', 'her', 'embarrassed', 'father', ',', 'who', 'was', 'listening', 'this', '.', 'her', 'mother', 'said', 'at', 'this', 'moment', ':', \"'you\", 'have', 'not', 'slapped', 'your', 'father', ',', 'but', 'you', 'have', 'actually', 'slapped', 'that', 'mentality', 'that', 'believes', 'in', 'differences', 'between', 'poor', 'and', 'rich', \"'\", '.', 'and', 'then', 'they', 'lived', 'happily', 'ever', 'after', '.']\n['heart', 'soul', 'different', 'social', 'class', 'daughter', 'business', 'tycoon', 'realistic', 'self-confident', 'bit', 'arrogant', 'poor', 'leisure', 'whereas', 'work', 'hotel', 'receptionist', 'fall', 'difference', 'arises', 'right', 'beginning', 'esp', 'criticised', 'social', 'status', 'earning', 'capacity', 'saying', 'daughter', 'cost', '7500', 'rupee', 'compare', 'monthly', 'salary', '750', 'rupee', 'struggle', 'threat', 'would', 'suicide', 'would', 'allowed', 'agreed', 'married', 'knew', 'really', 'happy', 'marriage', 'day', 'came', 'daughter', 'located', 'middle', 'class', 'offered', 'business', 'firm', 'denied', 'also', 'angried', 'telephone', 'installed', 'day', 'later', 'purchased', 'furniture', 'invited', 'guest', 'dance', 'party', 'came', 'back', 'became', 'angry', 'asked', 'leave', 'stay', 'left', 'tear', 'next', 'day', 'got', 'appointment', 'bigger', 'company', 'perhaps', 'hotel', 'murree', 'leave', 'city', 'immediately', 'leaving', 'city', 'tried', 'meet', 'entrance', 'met', 'misinform', 'want', 'face', 'decided', 'get', 'divorce', 'realised', 'made', 'big', 'mistake', 'marrying', 'poor', 'like', 'shocked', 'dishearted', 'left', 'city', 'side', 'waiting', 'would', 'come', 'back', 'take', 'back', 'deceived', 'visited', 'insulted', 'front', 'friend', 'going', 'divorce', 'shocked', 'extent', 'could', 'bear', 'pain', 'pregnancy', 'moved', 'hospital', 'city', 'gone', 'murree', 'driver', 'incidentally', 'injured', 'moved', 'hospital', 'admitted', 'child', 'delivery', 'hospital', 'lobby', 'found', 'unconscious', 'hospital', 'bed', 'interrupted', 'going', 'give', 'away', 'child', 'orphanage', 'refused', 'take', 'care', 'child', 'taken', 'sad', 'turn', 'point', 'esp', 'carrying', 'singing', 'sad', 'version', 'mujhe', 'dil', 'bhulana', 'remembering', 'happy', 'day', 'side', 'lied', 'also', 'died', 'tremendeous', 'shock', 'somewhat', 'lost', 'mental', 'balance', 'year', 'grown', 'teenage', 'sensitive', 'anxious', 'whereas', 'mental', 'instability', 'grew', 'day', 'day', 'asked', 'let', 'visit', 'grave', 'would', 'perhaps', 'help', 'next', 'day', 'visited', 'searched', 'graveyard', 'come', 'across', 'real', 'picking', 'flower', 'nothing', 'found', 'started', 'leaving', 'shahzeb', 'immediately', 'reached', 'asked', 'feature', 'hand', 'photo', 'shahzeb', 'recognised', 'ran', 'towards', 'road', 'come', 'started', 'singing', 'used', 'listen', 'mujhe', 'dil', 'bhulana', 'stopped', 'ran', 'towards', 'voice', 'coming', 'finally', 'reached', 'amazed', 'situation', 'asked', 'divorce', 'misunderstanding', 'truth', 'revealed', 'actually', 'planned', 'separation', 'lied', 'child', 'dead', 'turned', 'angriely', 'slapped', 'embarrassed', 'listening', 'moment', 'you', 'slapped', 'actually', 'slapped', 'mentality', 'belief', 'difference', 'poor', 'lived', 'happily']\nheart soul different social class daughter business tycoon realistic self-confident bit arrogant poor leisure whereas work hotel receptionist fall difference arises right beginning esp criticised social status earning capacity saying daughter cost 7500 rupee compare monthly salary 750 rupee struggle threat would suicide would allowed agreed married knew really happy marriage day came daughter located middle class offered business firm denied also angried telephone installed day later purchased furniture invited guest dance party came back became angry asked leave stay left tear next day got appointment bigger company perhaps hotel murree leave city immediately leaving city tried meet entrance met misinform want face decided get divorce realised made big mistake marrying poor like shocked dishearted left city side waiting would come back take back deceived visited insulted front friend going divorce shocked extent could bear pain pregnancy moved hospital city gone murree driver incidentally injured moved hospital admitted child delivery hospital lobby found unconscious hospital bed interrupted going give away child orphanage refused take care child taken sad turn point esp carrying singing sad version mujhe dil bhulana remembering happy day side lied also died tremendeous shock somewhat lost mental balance year grown teenage sensitive anxious whereas mental instability grew day day asked let visit grave would perhaps help next day visited searched graveyard come across real picking flower nothing found started leaving shahzeb immediately reached asked feature hand photo shahzeb recognised ran towards road come started singing used listen mujhe dil bhulana stopped ran towards voice coming finally reached amazed situation asked divorce misunderstanding truth revealed actually planned separation lied child dead turned angriely slapped embarrassed listening moment you slapped actually slapped mentality belief difference poor lived happily\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Μετατροπή σε TFIDF\n",
        "\n",
        "Το πρώτο βήμα θα είναι λοιπόν να μετατρέψετε το corpus σε αναπαράσταση tf-idf:"
      ],
      "metadata": {
        "id": "DD5KuSKrxQ8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# create sparse tf_idf representation\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(corpus)\n",
        "corpus_tf_idf_plain = vectorizer.transform(corpus)"
      ],
      "metadata": {
        "id": "s5YP6XCZPSyh",
        "execution": {
          "iopub.status.busy": "2022-12-30T16:13:27.005214Z",
          "iopub.execute_input": "2022-12-30T16:13:27.007171Z",
          "iopub.status.idle": "2022-12-30T16:13:28.975793Z",
          "shell.execute_reply.started": "2022-12-30T16:13:27.007133Z",
          "shell.execute_reply": "2022-12-30T16:13:28.974943Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Η συνάρτηση [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) όπως καλείται εδώ **δεν είναι βελτιστοποιημένη**. Οι επιλογές των μεθόδων και παραμέτρων της μπορεί να έχουν **δραματική επίδραση στην ποιότητα των συστάσεων** και είναι διαφορετικές για κάθε dataset. Επίσης, οι επιλογές αυτές έχουν πολύ μεγάλη επίδραση και στη **διαστατικότητα και όγκο των δεδομένων**. Η διαστατικότητα των δεδομένων με τη σειρά της θα έχει πολύ μεγάλη επίδραση στους **χρόνους εκπαίδευσης**, ιδιαίτερα στη δεύτερη εφαρμογή της άσκησης.\n",
        "\n",
        "Προσοχή: ο TfidfVectorizer έχει κάποιες δυνατότητες προεπεξεργασίας παρόποιες με αυτές που αναφέραμε στην προηγούμενη ενότητα. Ό,τι προεπεξεργασία μπορείτε να κάνετε που χρειάζεται ως είσοδο μόνο το κάθε document ξεχωριστά, κάντε την στο πρώτο βήμα της προεπεξεργασίας. Αν χρειάζεται γνώση των συνολικών στατιστικών της συλλογής, κάντε την με τον TfidfVectorizer."
      ],
      "metadata": {
        "id": "H-uRZK3EPSyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus_tf_idf_plain.shape)"
      ],
      "metadata": {
        "id": "y_Cw0brpnisF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Υλοποίηση του συστήματος συστάσεων\n",
        "\n",
        "Το σύστημα συστάσεων που θα υλοποιήσετε θα είναι μια συνάρτηση `content_recommender` με τρία ορίσματα: `target_movie`, `max_recommendations` και `corpus_type`. Στην `target_movie` περνάμε το ID μιας ταινίας-στόχου για την οποία μας ενδιαφέρει να βρούμε παρόμοιες ως προς το περιεχόμενο (τη σύνοψη) ταινίες, `max_recommendations` στο πλήθος.\n",
        "Υλοποιήστε τη συνάρτηση ως εξής: \n",
        "- για την ταινία-στόχο, θα υπολογίζετε την [ομοιότητα συνημιτόνου](https://en.wikipedia.org/wiki/Cosine_similarity) της με όλες τις ταινίες της συλλογής σας όπως αυτές αναπαριστώνται στο `corpus_type`.\n",
        "- με βάση την ομοιότητα συνημιτόνου που υπολογίσατε, δημιουργήστε ταξινομημένο πίνακα από το μεγαλύτερο στο μικρότερο, με τα indices (`ID`) των ταινιών. Παράδειγμα: αν η ταινία με index 1 έχει ομοιότητα συνημιτόνου με 3 ταινίες \\[0.2 1 0.6\\] (έχει ομοιότητα 1 με τον εαύτό της) ο ταξινομημένος αυτός πίνακας indices θα είναι \\[1 2 0\\].\n",
        "- Για την ταινία-στόχο εκτυπώστε: id, τίτλο, σύνοψη, κατηγορίες (categories)\n",
        "- Για τις `max_recommendations` ταινίες (πλην της ίδιας της ταινίας-στόχου που έχει cosine similarity 1 με τον εαυτό της) με τη μεγαλύτερη ομοιότητα συνημιτόνου (σε φθίνουσα σειρά), τυπώστε σειρά σύστασης (1 πιο κοντινή, 2 η δεύτερη πιο κοντινή κλπ), ομοιότητα συνημιτόνου, id, τίτλο, σύνοψη, και κατηγορίες (categories)\n"
      ],
      "metadata": {
        "id": "3LsmvSyVykTU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Βελτιστοποίηση του TfidfVectorizer\n",
        "\n",
        "Αφού υλοποιήσετε τη συνάρτηση `content_recommender` χρησιμοποιήστε την για να βελτιστοποιήσετε την `TfidfVectorizer`. Συγκεκριμένα, αρχικά μπορείτε να δείτε τι επιστρέφει το σύστημα για τυχαίες ταινίες-στόχους και για ένα μικρό `max_recommendations` (2 ή 3). Αν σε κάποιες ταινίες το σύστημα μοιάζει να επιστρέφει σημασιολογικά κοντινές ταινίες σημειώστε το `ID` τους. Δοκιμάστε στη συνέχεια να βελτιστοποιήσετε την `TfidfVectorizer` για τα συγκεκριμένα `ID` ώστε να επιστρέφονται σημασιολογικά κοντινές ταινίες για μεγαλύτερο αριθμό `max_recommendations`. Παράλληλα, όσο βελτιστοποιείτε την `TfidfVectorizer`, θα πρέπει να λαμβάνετε καλές συστάσεις για μεγαλύτερο αριθμό τυχαίων ταινιών. \n",
        "\n",
        "Ταυτόχρονα, μια αντίρροπη κατά κάποιο τρόπο κατεύθυνση της βελτιστοποίησης είναι να χρησιμοποιείτε τις παραμέτρους του `TfidfVectorizer` έτσι ώστε να μειώνονται οι διαστάσεις του Vector Space Model μέχρι το σημείο που θα αρχίσει να εμφανίζονται επιπτώσεις στην ποιότητα των συστάσεων. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8IvHkTUHyu78"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Βαθιά μάθηση: δημιουργία corpora με χρήση word emmbeddings\n",
        "\n",
        "Η προσέγγιση της κατασκευής μόνο μέσω tfidf του συστήματος συστάσεων έχει διάφορα μειονεκτήματα. Θα μας ενδιέφερε λοιπόν να δούμε αν μπορούμε να χρησιμοποιήσουμε για τις λέξεις **εμφυτεύματα (embeddings)**, δηλαδή τις πυκνές διανυσματικές αναπαραστάσεις για τις λέξεις που μας δίνει το μοντέλο **Word2Vec**\n",
        "\n",
        "Ωστόσο, το dataset της κάθε ομάδας είναι πολύ μικρό για να εξάγουμε τα δικά μας word embeddings (και να είναι καλά). Για το λόγο αυτό θα χρησιμοποιήσουμε τη μεθοδολογία της Βαθιάς Μάθησης που είναι η **Μεταφορά Μάθησης (Transfer Learning).**.\n",
        "\n",
        "Στη μεταφορά μάθησης ουσιαστικά μεταφέρουμε τη γνώση που έχει αποκτήσει ένα ήδη εκπαιδευμένο (και κατά κανόνα πολύ μεγάλο) σύστημα. Η μεταφορά γίνεται διαμέσου των τιμών των βαρών που έχει προσδιορίσει μετά το πέρας της εκπαίδευσης.\n",
        "\n",
        "Στην περίπτωσή μας, δεν μας ενδιαφέρουν τόσο τα ίδια τα βάρη των μοντέλων από τα οποία θα κάνουμε μεταφορά μάθησης. Κάτι τέτοιο θα μας ενδιέφερε αν π.χ. θέλαμε να συνεχίσουμε την εκπαίδευση στα δικά μας κείμενα. Μας ενδιαφέρουν όμως τα ίδια τα εμφυτεύματα, δηλαδή τα embeddings (διανύσματα διαστάσεων $m$) που έχει μάθει το νευρωνικό για το λεξιλόγιο του (vocabulary). To vocabulary σε τέτοια μεγάλα νευρωνικά θα είναι πιθανότατα υπερσύνολο του δικού μας."
      ],
      "metadata": {
        "id": "PSQ2tCs_hbaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CUZIjbXArvxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Μεταφορά μάθησης εμφυτευμάτων\n",
        "\n"
      ],
      "metadata": {
        "id": "6UFGxnb9iknm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Εμφυτεύματα του Gensim-data\n",
        "Το Gensim περιλαμβάνει αρκετά προεκπαιδευμένα μοντέλα εμφυτευμάτων Word2Vec. Με το επόμενο κελί παίρνουμε τη λίστα τους."
      ],
      "metadata": {
        "id": "G3Z28edwj4wF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U gensim\n",
        "import gensim.downloader\n",
        "print(list(gensim.downloader.info()['models'].keys()))"
      ],
      "metadata": {
        "id": "UX9ZkHSvi3Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Τα μοντέλα αυτά βρίσκονται στο [αποθετήριο Gensim-data](https://github.com/RaRe-Technologies/gensim-data) όπου μπορείτε να βρείτε και την τεκμηρίωσή τους. Η φόρτωση των μοντέλων αυτών γίνεται με τη συνάρτηση `gensim.downloader.load`."
      ],
      "metadata": {
        "id": "mS0XEVWUi_0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Άλλα εμφυτεύμαατα\n",
        "Μπορείτε να βρείτε προεκπαιδευμένα εμφυτεύματα και από πηγές εκτός του Gensim. Για παράδειγμα:\n",
        "\n",
        "- [Google News dataset](https://code.google.com/archive/p/word2vec/). Πρόκειται για προ-εκπαιδευμένα διανύσματα που έχουν εκπαιδευτεί σε μέρος του συνόλου δεδομένων Google News (περίπου 100 δισεκατομμύρια λέξεις). Το μοντέλο περιέχει διανύσματα 300 διαστάσεων για 3 εκατομμύρια λέξεις και φράσεις.\n",
        "- [Amazon BlazingText](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html). Το BlazingText δεν είναι μόνο προεκπαιδευμένα εμφυτεύματα αλλα και βελτιστοποιημένες υλοποιήσεις των αλγορίθμων Word2vec για την επεξεργασία κειμένου. Προυπόθεση είναι να δουλέψει κανείς στο SageMaker.\n",
        "\n",
        "Οι διαδικασίες φόρτωσης embeddings από εξωτερικά δεδομένα μπορεί να είναι ελαφρά διαφορετικές από αυτή του Gensim.\n",
        "\n"
      ],
      "metadata": {
        "id": "IfNs5fMAkADF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Παρατηρήσεις\n",
        "\n",
        "*   Επαναλαμβάνουμε ότι στην εργασία αυτή δεν μας ενδιαφέρουν τα ίδια τα μοντέλα αλλά το να μπορούμε για μία λέξη του λεξιλογίου μας να μπορούμε να βρούμε το embedding (διάνυσμα) που της αντιστοιχεί στο εκάστοτε προεκπαιδευμένο μοντέλο. \n",
        "\n",
        "*   Επίσης, δεν θα χρησιμοποιήσουμε την `Phrases` για να βρούμε bigrams στο dataset μας όπως θα ήταν το ορθότερο, καθώς αυτό θα απαιτούσε την συνέχιση της εκπαίδευσης του μοντέλου σε νέο λεξιλόγιο με πολύ λίγα νέα δεδομένα.\n"
      ],
      "metadata": {
        "id": "qkkZE41d_DjX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Δημιουργία corpora βασισμένων στα εμφυτεύματα\n",
        "\n",
        "Για να μπορέσουμε να ενσωματώσουμε τη γνώση που υπάρχει στα προεκπαιδευμένα εμφυτεύματα στο δικό μας corpus θα προχωρήσουμε όπως περιγράφεται ακολούθως.\n",
        "\n",
        "Για κάθε περιγραφή ταινίας $d$, η οποία αποτελείται από τις $N_d$ λέξεις $w_i$, το  $tfidf$ της κάθε λέξης $w_i$ δίνεται από τη σχέση:\n",
        "\n",
        "$$ tfidf(w_i) = tf(w_i,d) \\cdot idf(w_i)$$\n",
        "\n",
        "Ταυτόχρονα, σε κάθε λέξη $w_i$ αντιστοιχεί ένα διάνυσμα $W2V(w_i)$ από το μοντέλο εμφυτευμάτων που έχουμε εισάγει. Τα διανύσματα εμφυτευμάτων $W2V$ θα έχουν διάσταση $m$, ανάλογα το μοντέλο. \n",
        "\n",
        "Για κάθε ταινία d, μπορούμε να δημιουργήσουμε μια διανυσματική αναπαράσταση $W2V(d)$ διαστάσεων $m$ χρησιμοποιώντας το $tfidf(w_i)$ ως συντελεστή βαρύτητας για κάθε εμφύτευμα $W2V(w_i)$:\n",
        "\n",
        "$$ W2V(d) = \\frac{tfidf(w_1)\\cdot W2V(w_i) + tfidf(w_2)\\cdot W2V(w_2) + \\dotsc  + tfidf(w_{N_{d}})\\cdot W2V(w_{N_{d}})}{tfidf(w_1)+tfidf(w_2)+ \\dotsc + tfidf(w_{N_{d}})}$$\n"
      ],
      "metadata": {
        "id": "aF6bQOziqISL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### build_tfw2v\n",
        "\n",
        "Υλοποιήστε μια συνάρτηση `build_tfw2v` με ορίσματα:\n",
        "- `corpus` που θα είναι το προεπεξεργασμένο dataset σας,\n",
        "- `vectors` που θα είναι το μοντέλο που θα σας δίνει τα διανύσματα των εμφυτεύσεων vectors, και \n",
        "- `embeddings_size` που θα είναι η διάσταση των εμφυτευμάτων $m$.\n",
        "\n",
        "H συνάρτηση αυτή θα επιστρέφει ένα νέο corpus που θα είναι ένας πίνακας 5000 (όσες οι ταινίες σας) x $m$ (το η διάσταση των εμφυτευμάτων). Ανάλογα ποιο μοντέλο χρησιμποιείτε για transfer learning ο πίνακας αυτός θα είναι διαφορετικός.\n",
        "\n",
        "Μπορείτε πλεόν να καλείτε την `content_recommender` με διαφορετικά corpora στο όρισμα `corpus_type`. Σημειώστε ότι στο TFidfVectorizer χρησιμοποιουμε τη σειριακή μορφή των numpy arrays και ίσως σας χρησιμεύσει η `sparse.csr_matrix()` από την Scipy."
      ],
      "metadata": {
        "id": "Umbd7mv__be8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ανάλυση αποτελεσμάτων\n",
        "\n",
        "### Σύστημα συστάσεων βασισμένο μόνο στο tfidf\n",
        "\n",
        "- Σε markdown περιγράψτε τι προεπεξεργασία κάνετε στα κείμενα και γιατί.\n",
        "\n",
        "- Περιγράψτε πως προχωρήσατε στις επιλογές σας για τη βελτιστοποίηση της `TfidfVectorizer`. \n",
        "\n",
        "- [Cherry-picking:](https://www.wikiwand.com/en/Cherry_picking) Δώσετε παραδείγματα (IDs) από τη συλλογή σας που επιστρέφουν καλά αποτελέσματα μέχρι `max_recommendations` (τουλάχιστον 5) και σχολιάστε.\n",
        "\n",
        "- [Nit-picking:](https://en.wikipedia.org/wiki/Nitpicking) Δώστε παραδείγματα (IDs) από τη συλλογή σας που επιστρέφουν κακά αποτελέσματα και σχολιάστε.\n",
        "\n",
        "- Ποια είναι συνολικά τα πλεονεκτήματα και μειονεκτήματα ενός recommender βασισμένου στο tfidf;"
      ],
      "metadata": {
        "id": "NPVK7Z5c1p5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Σύγκριση και σχολιασμός με recommenders βασισμένων στο Word2Vec\n",
        "\n",
        "- Υλoποιήστε recommenders που βασίζονται σε μεταφορά μάθησης και εμφυτεύματα. Χρησιμοποιήστε παραδείγματα για να υποδείξετε δυνατά και αδύναμα σημεία τους.\n",
        "\n",
        "- Μπορείτε να σχολιάσετε τα recommenders που βασίζονται στο Word2Vec σε σχέση με το απλό μοντέλο tfidf, εξετάζοντας τις συστάσεις για ίδια ID.\n",
        "\n",
        "- Μπορείτε επίσης να εξετάσετε συγκριτικά τα Word2Vec recommenders μεταξύ τους και πάλι βασιζόμενοι σε παραδείγματα.\n",
        "\n",
        "- Οι παρατηρήσεις σας θα βασίζονται στην ανάλυση των ποιοτικών χαρακτηριστικών που είναι η σειρά και το σύνολο των συστάσεων. Ωστόσο, μπορείτε να συμπεριλάβετε και ποσοτικά χαρακτηριστικά όπως τους χρονους loading και συγκρότησης του corpus αλλά και της διαστατικότητας $m$.\n",
        "\n",
        "Χρησιμοποιήστε όποια μορφή reporting κρίνετε καταλληλότερη: κείμενο, πίνακες, διαγράμματα.\n"
      ],
      "metadata": {
        "id": "zPBW9WH56I-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Πρακτικό tip - persistence αντικειμένων με joblib.dump\n",
        "\n",
        "Καθώς στην δεύτερη εργασία καλείστε να δημιουργήσετε διάφορα corpora των οποίων η δημιουργία παίρνει χρόνο, υπάρχει ένας εύκολος τρόπος να αποθηκεύουμε μεταβλητές σε dump files και να τις διαβάζουμε απευθείας.\n",
        "\n",
        "H βιβλιοθήκη [joblib](https://pypi.python.org/pypi/joblib) της Python δίνει κάποιες εξαιρετικά χρήσιμες ιδιότητες στην ανάπτυξη κώδικα: pipelining, παραλληλισμό, caching και variable persistence. Τις τρεις πρώτες ιδιότητες τις είδαμε στην πρώτη άσκηση. Στην παρούσα άσκηση θα μας φανεί χρήσιμη η τέταρτη, το persistence των αντικειμένων. Συγκεκριμένα μπορούμε με:\n",
        "\n",
        "```python\n",
        "joblib.dump(my_object, 'my_object.pkl') \n",
        "```\n",
        "\n",
        "να αποθηκεύσουμε οποιοδήποτε αντικείμενο-μεταβλητή (εδώ το `my_object`) απευθείας πάνω στο filesystem ως αρχείο, το οποίο στη συνέχεια μπορούμε να ανακαλέσουμε ως εξής:\n",
        "\n",
        "```python\n",
        "my_object = joblib.load('my_object.pkl')\n",
        "```\n",
        "\n",
        "Μπορούμε έτσι να ανακαλέσουμε μεταβλητές ακόμα και αφού κλείσουμε και ξανανοίξουμε το notebook, χωρίς να χρειαστεί να ακολουθήσουμε ξανά όλα τα βήματα ένα - ένα για την παραγωγή τους, κάτι ιδιαίτερα χρήσιμο αν αυτή η διαδικασία είναι χρονοβόρα.\n",
        "\n",
        "Ας αποθηκεύσουμε το `corpus_tf_idf` και στη συνέχεια ας το ανακαλέσουμε."
      ],
      "metadata": {
        "id": "4irg4K-IPSym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(corpus_tf_idf, 'corpus_tf_idf.pkl') "
      ],
      "metadata": {
        "id": "aESOPYQaPSyo",
        "scrolled": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Μπορείτε με ένα απλό `!ls` να δείτε ότι το αρχείο `corpus_tf_idf.pkl` υπάρχει στο filesystem σας (== persistence):"
      ],
      "metadata": {
        "id": "7_rAEj5ZPSy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh"
      ],
      "metadata": {
        "id": "ZhwXmTEIPSy3",
        "scrolled": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "και μπορούμε να τα διαβάσουμε με `joblib.load`"
      ],
      "metadata": {
        "id": "cey5AbkO475S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_tf_idf = joblib.load('corpus_tf_idf.pkl')"
      ],
      "metadata": {
        "id": "DSJPTKY8PSyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Εφαρμογή 2.  Τοπολογική και σημασιολογική απεικόνιση της ταινιών με χρήση SOM\n",
        "<img src=\"https://i.imgur.com/Z4FdurD.jpg\" width=\"60%\">"
      ],
      "metadata": {
        "id": "zHOQtO83PSy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Δημιουργία dataset\n",
        "Στη δεύτερη εφαρμογή θα βασιστούμε στις τοπολογικές ιδιότητες των Self Organizing Maps (SOM) για να φτιάξουμε ενά χάρτη (grid) δύο διαστάσεων όπου θα απεικονίζονται όλες οι ταινίες της συλλογής της ομάδας με τρόπο χωρικά συνεκτικό ως προς το περιεχόμενο και κυρίως το είδος τους (ο παραπάνω χάρτης είναι ενδεικτικός, δεν αντιστοιχεί στο dataset μας). \n",
        "\n",
        "Διαλέξτε για την αναπαράσταση των documents αυτήν που πιστεύετε απέδωσε καλύτερα στο πρώτα σκέλος της άσκησης. Έστω ότι αυτή είναι η `my_best_corpus`.\n",
        "\n",
        "Η έτοιμη συνάρτηση `build_final_set` θα ενώσει την αναπαράσταση που θα της δώσετε ως όρισμα `mycorpus` με τις binarized κατηγορίες `catbins` των ταινιών ως επιπλέον κολόνες (χαρακτηριστικά). Συνεπώς, κάθε ταινία αναπαρίσταται στο Vector Space Model από τα χαρακτηριστικά της αναπαράστασης `mycorpus` και τις κατηγορίες της.\n",
        "\n",
        "Τέλος, η συνάρτηση δέχεται ένα ορισμα για το πόσες ταινίες να επιστρέψει, με default τιμή όλες τις ταινίες (5000). Αυτό είναι χρήσιμο για να μπορείτε αν θέλετε να φτιάχνετε μικρότερα σύνολα δεδομένων ώστε να εκπαιδεύεται ταχύτερα το SOM. \n",
        "\n",
        "Θα τρέχουμε τη συνάρτηση με `final_set = build_final_set(my_best_corpus)`."
      ],
      "metadata": {
        "id": "UB_clmizPSy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_final_set(mycorpus, doc_limit = 5000, tf_idf_only=False):\n",
        "    # convert sparse tf_idf to dense tf_idf representation\n",
        "    dense_tf_idf = mycorpus.toarray()[0:doc_limit,:]\n",
        "    if tf_idf_only:\n",
        "        # use only tf_idf\n",
        "        final_set = dense_tf_idf\n",
        "    else:\n",
        "        # append the binary categories features horizontaly to the (dense) tf_idf features\n",
        "        final_set = np.hstack((dense_tf_idf, catbins[0:doc_limit,:]))\n",
        "    # η somoclu θέλει δεδομ΄ένα σε float32\n",
        "    return np.array(final_set, dtype=np.float32)"
      ],
      "metadata": {
        "id": "U-FDDOkQPSzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Στο επόμενο κελί, τυπώνουμε τις διαστάσεις του τελικού dataset μας. **Χωρίς βελτιστοποίηση του TFIDF** θα έχουμε περίπου 50.000 χαρακτηριστικά και ο θα είναι ανέφικτο να προχωρήσουμε στην εκπαίδευση του SOM."
      ],
      "metadata": {
        "id": "KjvPPENS_dYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_set.shape"
      ],
      "metadata": {
        "id": "fvEgNn-L-jEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Εκπαίδευση χάρτη SOM\n",
        "\n",
        "Θα δουλέψουμε με τη βιβλιοθήκη SOM [\"Somoclu\"](http://somoclu.readthedocs.io/en/stable/index.html). Εισάγουμε τις somoclu και matplotlib και λέμε στη matplotlib να τυπώνει εντός του notebook (κι όχι σε pop up window)."
      ],
      "metadata": {
        "id": "8tikdip0PSzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install somoclu\n",
        "!pip install --upgrade somoclu\n",
        "# import sompoclu, matplotlib\n",
        "import somoclu\n",
        "import matplotlib\n",
        "# we will plot inside the notebook and not in separate window\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "oX9rzxGSPSzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Καταρχάς διαβάστε το [function reference](http://somoclu.readthedocs.io/en/stable/reference.html) του somoclu. Θα δoυλέψουμε με χάρτη τύπου planar, παραλληλόγραμμου σχήματος νευρώνων με τυχαία αρχικοποίηση (όλα αυτά είναι default). Μπορείτε να δοκιμάσετε διάφορα μεγέθη χάρτη ωστόσο όσο ο αριθμός των νευρώνων μεγαλώνει, μεγαλώνει και ο χρόνος εκπαίδευσης. Για το training δεν χρειάζεται να ξεπεράσετε τα 100 epochs. Σε γενικές γραμμές μπορούμε να βασιστούμε στις default παραμέτρους μέχρι να έχουμε τη δυνατότητα να οπτικοποιήσουμε και να αναλύσουμε ποιοτικά τα αποτελέσματα. Ξεκινήστε με ένα χάρτη 10 x 10, 100 epochs training και ένα υποσύνολο των ταινιών (π.χ. 2000). Χρησιμοποιήστε την `time` για να έχετε μια εικόνα των χρόνων εκπαίδευσης. "
      ],
      "metadata": {
        "id": "EqBfn0ijPSzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Best matching units\n",
        "\n",
        "Μετά από κάθε εκπαίδευση αποθηκεύστε σε μια μεταβλητή τα best matching units (bmus) για κάθε ταινία. Τα bmus μας δείχνουν σε ποιο νευρώνα ανήκει η κάθε ταινία. **Προσοχή: η σύμβαση των συντεταγμένων των νευρώνων στη Somoclu είναι (στήλη, γραμμή) δηλαδή το ανάποδο από την Python**. Με χρήση της [np.unique](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.unique.html) (μια πολύ χρήσιμη συνάρτηση στην άσκηση) αποθηκεύστε τα μοναδικά best matching units και τους δείκτες τους (indices) προς τις ταινίες. \n",
        "\n",
        "Σημειώστε ότι μπορεί να έχετε λιγότερα μοναδικά bmus από αριθμό νευρώνων γιατί μπορεί σε κάποιους νευρώνες να μην έχουν ανατεθεί ταινίες. Ως αριθμό νευρώνα θα θεωρήσουμε τον αριθμό γραμμής στον πίνακα μοναδικών bmus.\n"
      ],
      "metadata": {
        "id": "ntd2GE9SaHiS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Ομαδοποίηση (clustering)\n",
        "\n",
        "Τυπικά, η ομαδοποίηση σε ένα χάρτη SOM προκύπτει από το unified distance matrix (U-matrix): για κάθε κόμβο υπολογίζεται η μέση απόστασή του από τους γειτονικούς κόμβους. Εάν χρησιμοποιηθεί μπλε χρώμα στις περιοχές του χάρτη όπου η τιμή αυτή είναι χαμηλή (μικρή απόσταση) και κόκκινο εκεί που η τιμή είναι υψηλή (μεγάλη απόσταση), τότε μπορούμε να πούμε ότι οι μπλε περιοχές αποτελούν clusters και οι κόκκινες αποτελούν σύνορα μεταξύ clusters.\n",
        "\n",
        "To somoclu δίνει την επιπρόσθετη δυνατότητα να κάνουμε ομαδοποίηση των νευρώνων χρησιμοποιώντας οποιονδήποτε αλγόριθμο ομαδοποίησης του scikit-learn. Στην άσκηση θα χρησιμοποιήσουμε τον k-Means. Για τον αρχικό σας χάρτη δοκιμάστε ένα k=20 ή 25. Οι δύο προσεγγίσεις ομαδοποίησης είναι διαφορετικές, οπότε περιμένουμε τα αποτελέσματα να είναι κοντά αλλά όχι τα ίδια.\n"
      ],
      "metadata": {
        "id": "grzqcyHyaKdg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Αποθήκευση του SOM\n",
        "\n",
        "Επειδή η αρχικοποίηση του SOM γίνεται τυχαία και το clustering είναι και αυτό στοχαστική διαδικασία, οι θέσεις και οι ετικέτες των νευρώνων και των clusters θα είναι διαφορετικές κάθε φορά που τρέχετε τον χάρτη, ακόμα και με τις ίδιες παραμέτρους. Για να αποθηκεύσετε ένα συγκεκριμένο som και clustering χρησιμοποιήστε και πάλι την `joblib`. Μετά την ανάκληση ενός SOM θυμηθείτε να ακολουθήσετε τη διαδικασία για τα bmus.\n"
      ],
      "metadata": {
        "id": "2nupuqcuaMe3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Οπτικοποίηση U-matrix, clustering και μέγεθος clusters\n",
        "\n",
        "Για την εκτύπωση του U-matrix χρησιμοποιήστε τη `view_umatrix` με ορίσματα `bestmatches=True` και `figsize=(15, 15)` ή `figsize=(20, 20)`. Τα διαφορετικά χρώματα που εμφανίζονται στους κόμβους αντιπροσωπεύουν τα διαφορετικά clusters που προκύπτουν από τον k-Means. Μπορείτε να εμφανίσετε τη λεζάντα του U-matrix με το όρισμα `colorbar`. Μην τυπώνετε τις ετικέτες (labels) των δειγμάτων, είναι πολύ μεγάλος ο αριθμός τους.\n",
        "\n",
        "Για μια δεύτερη πιο ξεκάθαρη οπτικοποίηση του clustering τυπώστε απευθείας τη μεταβλητή `clusters`.\n",
        "\n",
        "Τέλος, χρησιμοποιώντας πάλι την `np.unique` (με διαφορετικό όρισμα) και την `np.argsort` (υπάρχουν και άλλοι τρόποι υλοποίησης) εκτυπώστε τις ετικέτες των clusters (αριθμοί από 0 έως k-1) και τον αριθμό των νευρώνων σε κάθε cluster, με φθίνουσα ή αύξουσα σειρά ως προς τον αριθμό των νευρώνων. Ουσιαστικά είναι ένα εργαλείο για να βρίσκετε εύκολα τα μεγάλα και μικρά clusters. \n",
        "\n",
        "Ακολουθεί ένα μη βελτιστοποιημένο παράδειγμα για τις τρεις προηγούμενες εξόδους:\n",
        "\n",
        "<img src=\"https://image.ibb.co/i0tsfR/umatrix_s.jpg\" width=\"35%\">\n",
        "<img src=\"https://image.ibb.co/nLgHEm/clusters.png\" width=\"35%\">\n",
        "\n"
      ],
      "metadata": {
        "id": "ejX0Qs18aRHU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Σημασιολογική ερμηνεία των clusters\n",
        "\n",
        "Προκειμένου να μελετήσουμε τις τοπολογικές ιδιότητες του SOM και το αν έχουν ενσωματώσει σημασιολογική πληροφορία για τις ταινίες διαμέσου της διανυσματικής αναπαράστασης του tf-idf, των εμφυτευμάτων και των κατηγοριών, χρειαζόμαστε ένα κριτήριο ποιοτικής επισκόπησης των clusters. \n",
        "\n",
        "Θα υλοποιήσουμε το εξής κριτήριο: Λαμβάνουμε όρισμα έναν αριθμό (ετικέτα) cluster. Για το cluster αυτό βρίσκουμε όλους τους νευρώνες που του έχουν ανατεθεί από τον k-Means. Για όλους τους νευρώνες αυτούς βρίσκουμε όλες τις ταινίες που τους έχουν ανατεθεί (για τις οποίες αποτελούν bmus). Για όλες αυτές τις ταινίες τυπώνουμε ταξινομημένη τη συνολική στατιστική όλων των ειδών (κατηγοριών) και τις συχνότητές τους. Αν το cluster διαθέτει καλή συνοχή και εξειδίκευση, θα πρέπει κάποιες κατηγορίες να έχουν σαφώς μεγαλύτερη συχνότητα από τις υπόλοιπες. Θα μπορούμε τότε να αναθέσουμε αυτήν/ές την/τις κατηγορία/ες ως ετικέτες κινηματογραφικού είδους στο cluster.\n",
        "\n",
        "Μπορείτε να υλοποιήσετε τη συνάρτηση αυτή όπως θέλετε. Μια πιθανή διαδικασία θα μπορούσε να είναι η ακόλουθη:\n",
        "\n",
        "1. Ορίζουμε συνάρτηση `print_categories_stats` που δέχεται ως είσοδο λίστα με ids ταινιών. Δημιουργούμε μια κενή λίστα συνολικών κατηγοριών. Στη συνέχεια, για κάθε ταινία επεξεργαζόμαστε το string `categories` ως εξής: δημιουργούμε μια λίστα διαχωρίζοντας το string κατάλληλα με την `split` και αφαιρούμε τα whitespaces μεταξύ ετικετών με την `strip`. Προσθέτουμε τη λίστα αυτή στη συνολική λίστα κατηγοριών με την `extend`. Τέλος χρησιμοποιούμε πάλι την `np.unique` για να μετρήσουμε συχνότητα μοναδικών ετικετών κατηγοριών και ταξινομούμε με την `np.argsort`. Τυπώνουμε τις κατηγορίες και τις συχνότητες εμφάνισης ταξινομημένα. Χρήσιμες μπορεί να σας φανούν και οι `np.ravel`, `np.nditer`, `np.array2string` και `zip`.\n",
        "\n",
        "2. Ορίζουμε τη βασική μας συνάρτηση `print_cluster_neurons_movies_report` που δέχεται ως όρισμα τον αριθμό ενός cluster. Με τη χρήση της `np.where` μπορούμε να βρούμε τις συντεταγμένες των bmus που αντιστοιχούν στο cluster και με την `column_stack` να φτιάξουμε έναν πίνακα bmus για το cluster. Προσοχή στη σειρά (στήλη - σειρά) στον πίνακα bmus. Για κάθε bmu αυτού του πίνακα ελέγχουμε αν υπάρχει στον πίνακα μοναδικών bmus που έχουμε υπολογίσει στην αρχή συνολικά και αν ναι προσθέτουμε το αντίστοιχο index του νευρώνα σε μια λίστα. Χρήσιμες μπορεί να είναι και οι `np.rollaxis`, `np.append`, `np.asscalar`. Επίσης πιθανώς να πρέπει να υλοποιήσετε ένα κριτήριο ομοιότητας μεταξύ ενός bmu και ενός μοναδικού bmu από τον αρχικό πίνακα bmus.\n",
        "\n",
        "3. Υλοποιούμε μια βοηθητική συνάρτηση `neuron_movies_report`. Λαμβάνει ένα σύνολο νευρώνων από την `print_cluster_neurons_movies_report` και μέσω της `indices` φτιάχνει μια λίστα με το σύνολο ταινιών που ανήκουν σε αυτούς τους νευρώνες. Στο τέλος καλεί με αυτή τη λίστα την `print_categories_stats` που τυπώνει τις στατιστικές των κατηγοριών.\n",
        "\n",
        "Μπορείτε βέβαια να προσθέσετε οποιαδήποτε επιπλέον έξοδο σας βοηθάει. Μια χρήσιμη έξοδος είναι πόσοι νευρώνες ανήκουν στο cluster και σε πόσους και ποιους από αυτούς έχουν ανατεθεί ταινίες.\n",
        "\n",
        "Θα επιτελούμε τη σημασιολογική ερμηνεία του χάρτη καλώντας την `print_cluster_neurons_movies_report` με τον αριθμός ενός cluster που μας ενδιαφέρει. \n",
        "\n",
        "Παράδειγμα εξόδου για ένα cluster (μη βελτιστοποιημένος χάρτης, ωστόσο βλέπετε ότι οι μεγάλες κατηγορίες έχουν σημασιολογική  συνάφεια):\n",
        "\n",
        "```\n",
        "Overall Cluster Genres stats:  \n",
        "[('\"Horror\"', 86), ('\"Science Fiction\"', 24), ('\"B-movie\"', 16), ('\"Monster movie\"', 10), ('\"Creature Film\"', 10), ('\"Indie\"', 9), ('\"Zombie Film\"', 9), ('\"Slasher\"', 8), ('\"World cinema\"', 8), ('\"Sci-Fi Horror\"', 7), ('\"Natural horror films\"', 6), ('\"Supernatural\"', 6), ('\"Thriller\"', 6), ('\"Cult\"', 5), ('\"Black-and-white\"', 5), ('\"Japanese Movies\"', 4), ('\"Short Film\"', 3), ('\"Drama\"', 3), ('\"Psychological thriller\"', 3), ('\"Crime Fiction\"', 3), ('\"Monster\"', 3), ('\"Comedy\"', 2), ('\"Western\"', 2), ('\"Horror Comedy\"', 2), ('\"Archaeology\"', 2), ('\"Alien Film\"', 2), ('\"Teen\"', 2), ('\"Mystery\"', 2), ('\"Adventure\"', 2), ('\"Comedy film\"', 2), ('\"Combat Films\"', 1), ('\"Chinese Movies\"', 1), ('\"Action/Adventure\"', 1), ('\"Gothic Film\"', 1), ('\"Costume drama\"', 1), ('\"Disaster\"', 1), ('\"Docudrama\"', 1), ('\"Film adaptation\"', 1), ('\"Film noir\"', 1), ('\"Parody\"', 1), ('\"Period piece\"', 1), ('\"Action\"', 1)]```\n",
        "   "
      ],
      "metadata": {
        "id": "fMO_KcQYaTv-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Tips για το SOM και το clustering\n",
        "\n",
        "- Για την ομαδοποίηση ένα U-matrix καλό είναι να εμφανίζει και μπλε-πράσινες περιοχές (clusters) και κόκκινες περιοχές (ορίων). Παρατηρήστε ποια σχέση υπάρχει μεταξύ αριθμού ταινιών στο final set, μεγέθους grid και ποιότητας U-matrix.\n",
        "- Για το k του k-Means προσπαθήστε να προσεγγίζει σχετικά τα clusters του U-matrix (όπως είπαμε είναι διαφορετικοί μέθοδοι clustering). Μικρός αριθμός k δεν θα σέβεται τα όρια. Μεγάλος αριθμός θα δημιουργεί υπο-clusters εντός των clusters που φαίνονται στο U-matrix. Το τελευταίο δεν είναι απαραίτητα κακό, αλλά μεγαλώνει τον αριθμό clusters που πρέπει να αναλυθούν σημασιολογικά.\n",
        "- Σε μικρούς χάρτες και με μικρά final sets δοκιμάστε διαφορετικές παραμέτρους για την εκπαίδευση του SOM. Σημειώστε τυχόν παραμέτρους που επηρεάζουν την ποιότητα του clustering για το dataset σας ώστε να τις εφαρμόσετε στους μεγάλους χάρτες.\n",
        "- Κάποια τοπολογικά χαρακτηριστικά εμφανίζονται ήδη σε μικρούς χάρτες. Κάποια άλλα χρειάζονται μεγαλύτερους χάρτες. Δοκιμάστε μεγέθη 20x20, 25x25 ή και 30x30 και αντίστοιχη προσαρμογή των k. Όσο μεγαλώνουν οι χάρτες, μεγαλώνει η ανάλυση του χάρτη αλλά μεγαλώνει και ο αριθμός clusters που πρέπει να αναλυθούν.\n"
      ],
      "metadata": {
        "id": "lq4QrImhaa7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Ανάλυση τοπολογικών ιδιοτήτων χάρτη SOM\n",
        "\n",
        "Μετά το πέρας της εκπαίδευσης και του clustering θα έχετε ένα χάρτη με τοπολογικές ιδιότητες ως προς τα είδη των ταίνιών της συλλογής σας, κάτι αντίστοιχο με την εικόνα στην αρχή της Εφαρμογής 2 αυτού του notebook. Η συγκεκριμένη εικόνα είναι μόνο για εικονογράφιση, δεν είναι χάρτης SOM καιδεν έχει καμία σχέση με τη συλλογή δεδομένων και τις κατηγορίες μας.\n",
        "\n",
        "Για τον τελικό χάρτη SOM που θα παράξετε για τη συλλογή σας, αναλύστε σε markdown με συγκεκριμένη αναφορά σε αριθμούς clusters και τη σημασιολογική ερμηνεία τους τις εξής τρεις τοπολογικές ιδιότητες του SOM: \n",
        "\n",
        "1. Δεδομένα που έχουν μεγαλύτερη πυκνότητα πιθανότητας στο χώρο εισόδου τείνουν να απεικονίζονται με περισσότερους νευρώνες στο χώρο μειωμένης διαστατικότητας. Δώστε παραδείγματα από συχνές και λιγότερο συχνές κατηγορίες ταινιών. Χρησιμοποιήστε τις στατιστικές των κατηγοριών στη συλλογή σας και τον αριθμό κόμβων που χαρακτηρίζουν.\n",
        "2. Μακρινά πρότυπα εισόδου τείνουν να απεικονίζονται απομακρυσμένα στο χάρτη. Υπάρχουν χαρακτηριστικές κατηγορίες ταινιών που ήδη από μικρούς χάρτες τείνουν να τοποθετούνται σε διαφορετικά ή απομονωμένα σημεία του χάρτη.\n",
        "3. Κοντινά πρότυπα εισόδου τείνουν να απεικονίζονται κοντά στο χάρτη. Σε μεγάλους χάρτες εντοπίστε είδη ταινιών και κοντινά τους υποείδη.\n",
        "\n",
        "Προφανώς τοποθέτηση σε 2 διαστάσεις που να σέβεται μια απόλυτη τοπολογία δεν είναι εφικτή, αφενός γιατί δεν υπάρχει κάποια απόλυτη εξ ορισμού για τα κινηματογραφικά είδη ακόμα και σε πολλές διαστάσεις, αφετέρου γιατί πραγματοποιούμε μείωση διαστατικότητας.\n",
        "\n",
        "Εντοπίστε μεγάλα clusters και μικρά clusters που δεν έχουν σαφή χαρακτηριστικά. Εντοπίστε clusters συγκεκριμένων ειδών που μοιάζουν να μην έχουν τοπολογική συνάφεια με γύρω περιοχές. Προτείνετε πιθανές ερμηνείες.\n",
        "\n",
        "\n",
        "Τέλος, εντοπίστε clusters που έχουν κατά την άποψή σας ιδιαίτερο ενδιαφέρον στη συλλογή της ομάδας σας (data exploration / discovery value) και σχολιάστε.\n"
      ],
      "metadata": {
        "id": "x4IUl8O8ayVf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Τελική παράδοση άσκησης\n",
        "\n",
        "- Θα παραδώσετε στο helios το παρόν notebook επεξεργασμένο ή ένα ή δύο νέα zipαρισμένα με τις απαντήσεις σας για τα ζητούμενα και των δύο εφαρμογών. \n",
        "- Θυμηθείτε ότι η ανάλυση του χάρτη στο markdown με αναφορά σε αριθμούς clusters πρέπει να αναφέρεται στον τελικό χάρτη με τα κελιά ορατά που θα παραδώσετε αλλιώς ο χάρτης που θα προκύψει θα είναι διαφορετικός και τα labels των clusters δεν θα αντιστοιχούν στην ανάλυσή σας. \n",
        "- Μην ξεχάσετε στην αρχή ένα κελί markdown με **τα στοιχεία της ομάδας σας**."
      ],
      "metadata": {
        "id": "tYjxGR5DawIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table>\n",
        "  <tr><td align=\"center\">\n",
        "    <font size=\"4\">Παρακαλούμε διατρέξτε βήμα-βήμα το notebook για να μην ξεχάσετε παραδοτέα</font>\n",
        "</td>\n",
        "  </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "UHhCkvxjnitd"
      }
    }
  ]
}